import os
import datetime
import logging
import aiofiles
import markdown
import json
import hashlib
from typing import List, Dict, Any, Optional
from jinja2 import Environment, FileSystemLoader, select_autoescape

# Get logger from config
logger = logging.getLogger("gnosis_wraith")
# Storage paths now handled by storage service
STORAGE_PATH = os.environ.get('STORAGE_PATH', os.path.join(os.path.dirname(os.path.dirname(__file__)), 'storage'))

ENABLE_USER_PARTITIONING = os.environ.get('ENABLE_USER_PARTITIONING', 'false').lower() == 'true'

def get_user_reports_dir(email: Optional[str] = None) -> str:
    """Get user-specific reports directory."""
    # For now, always return the standard reports directory
    return REPORTS_DIR

def get_user_screenshots_dir(email: Optional[str] = None) -> str:
    """Get user-specific screenshots directory."""
    # For now, always return the standard screenshots directory
    return SCREENSHOTS_DIR

# Set up Jinja2 template environment
# Get the current file's directory (/app/server or /path/to/gnosis-wraith/server)
current_dir = os.path.dirname(os.path.abspath(__file__))

# In Docker container: /app/core -> need /app/web/templates
# In local dev: /path/to/gnosis-wraith/core -> need /path/to/gnosis-wraith/web/templates
if '/app/core' in current_dir:
    # Docker environment - templates are at /app/web/templates
    template_dir = '/app/web/templates'
else:
    # Local development - calculate relative path
    project_root = os.path.dirname(current_dir)  # Go up from core/ to project root
    template_dir = os.path.join(project_root, 'web', 'templates')

jinja_env = Environment(
    loader=FileSystemLoader(template_dir),
    autoescape=select_autoescape(['html', 'xml'])
)

def generate_markdown_report(title: str, crawl_results: List[Dict[str, Any]]) -> str:
    """Generate a markdown report from crawl results, including LLM summaries if available."""
    # Add logging for debugging
    logger.info(f"Generating markdown report with title: {title}")
    logger.info(f"Crawl results type: {type(crawl_results)}, length: {len(crawl_results)}")
    
    # Validate crawl_results
    if not isinstance(crawl_results, list):
        logger.error(f"crawl_results is not a list, but {type(crawl_results)}")
        crawl_results = [crawl_results] if crawl_results is not None else []
    
    # Log the first result for debugging
    if crawl_results:
        logger.info(f"First result type: {type(crawl_results[0])}")
        if isinstance(crawl_results[0], dict):
            logger.info(f"First result keys: {list(crawl_results[0].keys())}")
        else:
            logger.error(f"First result is not a dictionary but {type(crawl_results[0])}")
    
    md = f"# {title}\n\n"
    md += f"*Generated on {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n"
    
    # Add executive summary if available in results
    has_llm_summary = False
    for result in crawl_results:
        if isinstance(result, dict) and 'llm_summary' in result:
            has_llm_summary = True
            break
    
    if has_llm_summary:
        md += f"## Executive Summary\n\n"
        md += f"*This summary was generated by AI based on the collected content*\n\n"
        
        # Try to find an overall summary if one was generated
        overall_summary = None
        for result in crawl_results:
            if isinstance(result, dict) and 'overall_summary' in result:
                overall_summary = result.get('overall_summary')
                break
                
        if overall_summary:
            md += f"{overall_summary}\n\n"
        else:
            # Create a simple summary listing the pages analyzed
            md += f"This report contains analysis of {len(crawl_results)} URLs:\n\n"
            for result in crawl_results:
                if isinstance(result, dict) and 'error' not in result:
                    # Check if this is an uploaded file or URL
                    if 'original_filename' in result:
                        url_value = f"Local File: {result.get('original_filename', 'Unknown File')}"
                    else:
                        url_value = result.get('url', 'Unknown URL')
                    
                    # Ensure url_value is a string
                    if not isinstance(url_value, str):
                        url_value = str(url_value)
                    
                    # Use original filename as title if available
                    title_value = result.get('original_filename', result.get('title', 'Untitled'))
                    md += f"- {title_value}: {url_value}\n"
            md += "\n"
        
        md += "---\n\n"
    
    for i, result in enumerate(crawl_results, 1):
        # Skip if result is not a dictionary
        if not isinstance(result, dict):
            logger.error(f"Result at index {i-1} is not a dictionary but {type(result)}")
            continue
            
        # Use original filename as title if available
        title_value = result.get('original_filename', result.get('title', 'Untitled'))
        md += f"## {i}. {title_value}\n\n"
        
        # Check if this is an uploaded file or URL
        if 'original_filename' in result:
            file_value = result.get('original_filename', 'Unknown File')
            md += f"**File**: {file_value}\n\n"
        else:
            # Ensure URL is a string
            url_value = result.get('url', 'Unknown URL')
            if not isinstance(url_value, str):
                url_value = str(url_value)
            md += f"**URL**: {url_value}\n\n"
        
        # Add JavaScript setting information if available
        if 'javascript_enabled' in result:
            js_enabled = result.get('javascript_enabled', False)
            md += f"**JavaScript**: {'Enabled' if js_enabled else 'Disabled'}\n\n"
        
        if 'error' in result:
            error_value = result.get('error', 'Unknown error')
            if not isinstance(error_value, str):
                error_value = str(error_value)
            md += f"**Error**: {error_value}\n\n"
        else:
            # Add screenshot as image - use relative path
            if 'screenshot' in result:
                screenshot_value = result.get('screenshot')
                if not isinstance(screenshot_value, str):
                    logger.error(f"Screenshot value is not a string but {type(screenshot_value)}")
                    screenshot_value = str(screenshot_value)
                
                logger.info(f"Adding screenshot to report: {screenshot_value}")
                try:
                    # Get original filename from URL or path
                    original_filename = None
                    
                    # Try to get from metadata first if available
                    if 'original_filename' in result:
                        original_filename = result.get('original_filename')
                        if not isinstance(original_filename, str):
                            original_filename = str(original_filename)
                    else:
                        # Extract from path or URL as fallback
                        original_filename = os.path.basename(screenshot_value)
                    
                    # Fix paths to use storage directory properly
                    # First get absolute path to the image
                    if is_running_in_cloud():
                        # In cloud, construct GCS URL or use bucket path
                        storage_path = f"/storage/{screenshot_value}"
                    else:
                        # In local dev, use path relative to the storage root
                        storage_root = os.environ.get('STORAGE_PATH', os.path.join(os.path.dirname(os.path.dirname(__file__)), 'storage'))
                        storage_path = os.path.join(storage_root, screenshot_value)
                        # Make sure path exists
                        if not os.path.exists(storage_path):
                            logger.warning(f"Image path not found: {storage_path}, trying relative path")
                            storage_path = screenshot_value
                    
                    # Use a web-accessible path for the image in the report
                    web_path = f"/screenshots/{os.path.basename(screenshot_value)}"
                    
                    md += f"**Screenshot**:\n\n"
                    md += f"![{original_filename}]({web_path})\n\n"
                except Exception as e:
                    logger.error(f"Error adding screenshot to report: {str(e)}")
                    md += f"**Screenshot**: Error including screenshot: {str(e)}\n\n"
            
            # Add LLM summary if available
            if 'llm_summary' in result:
                summary_value = result.get('llm_summary')
                if not isinstance(summary_value, str):
                    logger.error(f"LLM summary is not a string but {type(summary_value)}")
                    summary_value = str(summary_value)
                md += f"**AI Summary**:\n\n"
                md += f"{summary_value}\n\n"
            
            # Add LLM error if there was an issue with AI processing
            if 'llm_error' in result:
                error_value = result.get('llm_error')
                if not isinstance(error_value, str):
                    error_value = str(error_value)
                md += f"**AI Processing Error**:\n\n"
                md += f"```\n{error_value}\n```\n\n"
            
            # Use the enhanced markdown content if available
            if 'fit_markdown_content' in result and result.get('fit_markdown_content'):
                content = result.get('fit_markdown_content')
                if not isinstance(content, str):
                    logger.error(f"fit_markdown_content is not a string but {type(content)}")
                    content = str(content)
                md += f"**Content (Enhanced Markdown)**:\n\n"
                md += f"{content}\n\n"
            elif 'markdown_content' in result and result.get('markdown_content'):
                content = result.get('markdown_content')
                if not isinstance(content, str):
                    logger.error(f"markdown_content is not a string but {type(content)}")
                    content = str(content)
                md += f"**Content (Markdown)**:\n\n"
                md += f"{content}\n\n"
            # Fall back to filtered_content if markdown isn't available
            elif 'filtered_content' in result and result.get('filtered_content'):
                content = result.get('filtered_content')
                if not isinstance(content, str):
                    logger.error(f"filtered_content is not a string but {type(content)}")
                    content = str(content)
                md += f"**Content (Extracted Text)**:\n\n"
                md += f"```\n{content}\n```\n\n"
            # Fallback to extracted text as a last resort
            elif 'extracted_text' in result:
                content = result.get('extracted_text')
                if not isinstance(content, str):
                    logger.error(f"extracted_text is not a string but {type(content)}")
                    content = str(content)
                
                # Check if OCR was enabled or disabled
                ocr_status = ""
                if 'ocr_enabled' in result:
                    ocr_status = " (OCR Enabled)" if result.get('ocr_enabled') else " (OCR Disabled)"
                
                md += f"**Extracted Text{ocr_status}**:\n\n"
                md += f"```\n{content}\n```\n\n"
        
        md += "---\n\n"
    
    # Add metadata section with information about the processing
    md += f"## Metadata\n\n"
    md += f"- **Total URLs Processed**: {len(crawl_results)}\n"
    
    # Include information about the LLM provider used if available
    try:
        llm_provider = None
        for result in crawl_results:
            if isinstance(result, dict) and 'llm_provider' in result:
                llm_provider = result.get('llm_provider')
                if not isinstance(llm_provider, str):
                    llm_provider = str(llm_provider)
                break
                
        if llm_provider:
            md += f"- **AI Provider**: {llm_provider}\n"
    except Exception as e:
        logger.error(f"Error adding LLM provider to metadata: {str(e)}")
    
    md += f"- **Generated By**: Gnosis Wraith with Enhanced Markdown Conversion\n"
    
    # Normalize excessive newlines (more than 2) to a maximum of 2
    import re
    md = re.sub(r'\n{3,}', '\n\n', md)
    
    logger.info("Markdown report generation completed successfully")
    return md

def is_running_in_cloud():
    """Detect if running in Google Cloud environment."""
    return os.environ.get('RUNNING_IN_CLOUD', '').lower() == 'true'

async def save_markdown_report(title: str, crawl_results: List[Dict[str, Any]], email: Optional[str] = None) -> str:
    """Save a markdown report to disk and return the file path."""
    # Ensure title is a string
    if not isinstance(title, str):
        logger.error(f"Title is not a string but {type(title)}")
        title = str(title)
    
    # Ensure crawl_results is properly structured
    if not isinstance(crawl_results, list):
        logger.error(f"crawl_results is not a list but {type(crawl_results)}")
        if crawl_results is None:
            crawl_results = []
        else:
            crawl_results = [crawl_results]
    
    # Ensure each result in crawl_results is a dictionary
    for i, result in enumerate(crawl_results):
        if not isinstance(result, dict):
            logger.error(f"Result at index {i} is not a dictionary but {type(result)}")
            crawl_results[i] = {"url": f"Item {i}", "error": f"Invalid result type: {type(result)}"}
        else:
            # Ensure all values in the dictionary are of expected types
            for key, value in list(result.items()):
                # These fields should always be strings
                if key in ['filtered_content', 'extracted_text', 'markdown_content', 'fit_markdown_content', 'title', 'url']:
                    if value is not None and not isinstance(value, str):
                        logger.error(f"Key '{key}' has invalid type {type(value)} at index {i}")
                        result[key] = str(value)
    
    report_content = generate_markdown_report(title, crawl_results)

    # Generate filename based on title and timestamp
    # Since filename_utils.py was removed, implement inline
    import string
    
    # Extract primary URL from crawl results if available
    primary_url = None
    for result in crawl_results:
        if isinstance(result, dict) and 'url' in result:
            primary_url = result.get('url')
            break
    
    if primary_url:
        # Create hash-based filename from URL
        url_hash = hashlib.sha256(primary_url.encode()).hexdigest()[:8]
        # Clean title for filename
        valid_chars = string.ascii_letters + string.digits + '-_'
        safe_title = ''.join(c if c in valid_chars else '_' for c in title)
        filename = f"{safe_title}_{url_hash}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    else:
        # Fallback to old naming if no URL found
        valid_chars = string.ascii_letters + string.digits + '-_'
        safe_title = ''.join(c if c in valid_chars else '_' for c in title)
        filename = f"{safe_title}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"


    # Get user-specific reports directory
    user_reports_dir = get_user_reports_dir(email)
    
    # Ensure directory exists
    os.makedirs(user_reports_dir, exist_ok=True)
    
    # Create full path with user directory
    report_path = os.path.join(user_reports_dir, filename)
    
    async with aiofiles.open(report_path, 'w') as f:
        await f.write(report_content)
    
    logger.info(f"Saved report to user directory: {report_path}")
    return report_path

def generate_json_report(title: str, crawl_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Generate a JSON report from crawl results, including LLM summaries if available."""
    # Add logging for debugging
    logger.info(f"Generating JSON report with title: {title}")
    logger.info(f"Crawl results type: {type(crawl_results)}, length: {len(crawl_results)}")
    
    # Validate crawl_results
    if not isinstance(crawl_results, list):
        logger.error(f"crawl_results is not a list, but {type(crawl_results)}")
        crawl_results = [crawl_results] if crawl_results is not None else []
    
    # Create base JSON structure
    json_report = {
        "title": title,
        "generated_at": datetime.datetime.now().isoformat(),
        "pages": [],
        "metadata": {
            "total_pages": len(crawl_results),
            "generated_by": "Gnosis Wraith with JSON Conversion"
        }
    }
    
    # Check for overall summary in any of the results
    for result in crawl_results:
        if isinstance(result, dict) and 'overall_summary' in result:
            json_report["overall_summary"] = result.get('overall_summary')
            break
    
    # Add LLM provider info if available
    for result in crawl_results:
        if isinstance(result, dict) and 'llm_provider' in result:
            json_report["metadata"]["ai_provider"] = result.get('llm_provider')
            break
    
    # Process each page
    for i, result in enumerate(crawl_results, 1):
        # Skip if result is not a dictionary
        if not isinstance(result, dict):
            logger.error(f"Result at index {i-1} is not a dictionary but {type(result)}")
            continue
        
        page_data = {
            "id": i,
            "title": result.get('original_filename', result.get('title', 'Untitled'))
        }
        
        # Check if this is an uploaded file or URL
        if 'original_filename' in result:
            page_data["file"] = result.get('original_filename')
            page_data["type"] = "file"
        else:
            page_data["url"] = result.get('url', 'Unknown URL')
            page_data["type"] = "url"
            
        # Add JavaScript setting information if available
        if 'javascript_enabled' in result:
            page_data["javascript_enabled"] = result.get('javascript_enabled', False)
        
        # Add error if present
        if 'error' in result:
            page_data["error"] = result.get('error')
        else:
            # Add screenshot info
            if 'screenshot' in result:
                screenshot_value = result.get('screenshot')
                if not isinstance(screenshot_value, str):
                    screenshot_value = str(screenshot_value)
                
                # Use a web-accessible path for the image in the report
                web_path = f"/screenshots/{os.path.basename(screenshot_value)}"
                page_data["screenshot"] = web_path
            
            # Add LLM summary if available
            if 'llm_summary' in result:
                page_data["ai_summary"] = result.get('llm_summary')
            
            # Add LLM error if there was an issue with AI processing
            if 'llm_error' in result:
                page_data["ai_error"] = result.get('llm_error')
            
            # Add content info - trying different fields in order of preference
            if 'fit_markdown_content' in result and result.get('fit_markdown_content'):
                page_data["content"] = result.get('fit_markdown_content')
                page_data["content_type"] = "enhanced_markdown"
            elif 'markdown_content' in result and result.get('markdown_content'):
                page_data["content"] = result.get('markdown_content')
                page_data["content_type"] = "markdown"
            elif 'filtered_content' in result and result.get('filtered_content'):
                page_data["content"] = result.get('filtered_content')
                page_data["content_type"] = "filtered_text"
            elif 'extracted_text' in result:
                page_data["content"] = result.get('extracted_text')
                page_data["content_type"] = "raw_text"
                
                # Check if OCR was enabled
                if 'ocr_enabled' in result:
                    page_data["ocr_enabled"] = result.get('ocr_enabled')
        
        # Add the page data to the report
        json_report["pages"].append(page_data)
    
    return json_report

async def save_json_report(title: str, crawl_results: List[Dict[str, Any]], email: Optional[str] = None) -> str:
    """Save a JSON report to disk and return the file path."""
    # Ensure title is a string
    if not isinstance(title, str):
        logger.error(f"Title is not a string but {type(title)}")
        title = str(title)
    
    # Ensure crawl_results is properly structured
    if not isinstance(crawl_results, list):
        logger.error(f"crawl_results is not a list but {type(crawl_results)}")
        if crawl_results is None:
            crawl_results = []
        else:
            crawl_results = [crawl_results]
    
    # Generate the JSON report
    json_report = generate_json_report(title, crawl_results)

    # Generate filename based on title and timestamp
    # Since filename_utils.py was removed, implement inline
    import string
    
    # Extract primary URL from crawl results if available
    primary_url = None
    for result in crawl_results:
        if isinstance(result, dict) and 'url' in result:
            primary_url = result.get('url')
            break
    
    if primary_url:
        # Create hash-based filename from URL
        url_hash = hashlib.sha256(primary_url.encode()).hexdigest()[:8]
        # Clean title for filename
        valid_chars = string.ascii_letters + string.digits + '-_'
        safe_title = ''.join(c if c in valid_chars else '_' for c in title)
        filename = f"{safe_title}_{url_hash}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    else:
        # Fallback to old naming if no URL found
        valid_chars = string.ascii_letters + string.digits + '-_'
        safe_title = ''.join(c if c in valid_chars else '_' for c in title)
        filename = f"{safe_title}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"


    # Get user-specific reports directory
    user_reports_dir = get_user_reports_dir(email)
    
    # Ensure directory exists
    os.makedirs(user_reports_dir, exist_ok=True)
    
    # Create full path with user directory
    report_path = os.path.join(user_reports_dir, filename)
    
    # Save the JSON file
    async with aiofiles.open(report_path, 'w') as f:
        await f.write(json.dumps(json_report, indent=2))
    
    logger.info(f"Saved JSON report to user directory: {report_path}")
    return report_path

async def convert_markdown_to_html(markdown_file: str) -> str:
    """Convert a markdown file to HTML with a modern, sleek design similar to about.html."""
    logger.info(f"Converting markdown file to HTML: {markdown_file}")
    
    # Validate input
    if not isinstance(markdown_file, str):
        logger.error(f"markdown_file is not a string but {type(markdown_file)}")
        markdown_file = str(markdown_file)
    
    # Create HTML filename
    html_file = f"{os.path.splitext(markdown_file)[0]}.html"
    logger.info(f"HTML output will be: {html_file}")
    
    try:
        # Read markdown content
        async with aiofiles.open(markdown_file, 'r', encoding='utf-8') as f:
            md_content = await f.read()
            logger.info(f"Read {len(md_content)} bytes from markdown file")
        
        # Process image references for lazy loading
        import re
        
        # Find all image references in the markdown
        image_refs = re.findall(r'!\[(.*?)\]\((.*?)\)', md_content)
        logger.info(f"Found {len(image_refs)} image references in markdown")
        
        # Track if we have screenshots to lazy load
        has_screenshots = False
        
        # Process each image reference for validation and lazy loading prep
        for alt_text, image_path in image_refs:
            logger.info(f"Found image reference in markdown: {alt_text} at path {image_path}")
            
            # Check if this is a screenshot (typically start with /screenshots/)
            if '/screenshots/' in image_path:
                has_screenshots = True
            
            # Don't check paths that start with / as they're web paths
            if not image_path.startswith('/'):
                # Check if the image path exists or needs fixing
                if not os.path.isabs(image_path):
                    # Try to resolve relative to the REPORTS_DIR
                    potential_path = os.path.join(os.path.dirname(markdown_file), image_path)
                    if os.path.exists(potential_path):
                        logger.info(f"Image path is valid: {potential_path}")
                    else:
                        logger.warning(f"Image path may be invalid: {potential_path}")
        
        # Convert all screenshots to lazy-loaded format
        if has_screenshots:
            # Replace standard markdown image syntax with custom lazy-loaded version for screenshots
            md_content = re.sub(
                r'!\[(.*?)\]\((/screenshots/[^)]+)\)',
                r'<div class="screenshot-container" data-alt="\1" data-src="\2">\n'
                r'<div class="screenshot-placeholder">\n'
                r'<i class="fas fa-image"></i>\n'
                r'<span>Click to load screenshot</span>\n'
                r'</div>\n'
                r'</div>',
                md_content
            )
        
        # Add extensions for better markdown rendering
        logger.info("Converting markdown to HTML with extensions")
        try:
            html = markdown.markdown(md_content, extensions=['tables', 'fenced_code', 'codehilite'])
            logger.info(f"Converted to HTML, size: {len(html)} bytes")
        except Exception as md_error:
            logger.error(f"Error converting markdown to HTML: {str(md_error)}")
            html = f"<p>Error converting markdown to HTML: {str(md_error)}</p><pre>{md_content}</pre>"
    except Exception as read_error:
        logger.error(f"Error reading markdown file: {str(read_error)}")
        return markdown_file

    try:
        # Extract metadata from the content for the header
        # Get report information for metadata display
        report_title = os.path.basename(markdown_file).replace('.md', '')
        
        # First try to extract URL from content using explicit markers
        url_match = re.search(r'\*\*URL\*\*: (https?://[^\s\n]+)', md_content)
        base_url = url_match.group(1) if url_match else None
        
        # Convert relative links to absolute URLs using base_url
        if base_url and html:
            from urllib.parse import urljoin, urlparse
            
            def convert_relative_links(html_content, base_url):
                """Convert relative links in HTML to absolute URLs"""
                try:
                    # Parse the base URL to get domain info
                    parsed_base = urlparse(base_url)
                    base_domain = f"{parsed_base.scheme}://{parsed_base.netloc}"
                    
                    # Convert relative href links to absolute
                    html_content = re.sub(
                        r'href="(/[^"]*)"',  # Match href="/relative/path"
                        lambda m: f'href="{urljoin(base_domain, m.group(1))}"',
                        html_content
                    )
                    
                    # Convert relative href links without leading slash
                    html_content = re.sub(
                        r'href="([^"]*(?<!https?://[^"]*)[^"]*)"',  # Match href="relative/path" but not full URLs
                        lambda m: f'href="{urljoin(base_url, m.group(1))}"' if not m.group(1).startswith(('http://', 'https://', 'mailto:', 'tel:', '#')) else m.group(0),
                        html_content
                    )
                    
                    # Convert relative src links for images
                    html_content = re.sub(
                        r'src="(/[^"]*)"',  # Match src="/relative/path"
                        lambda m: f'src="{urljoin(base_domain, m.group(1))}"',
                        html_content
                    )
                    
                    return html_content
                except Exception as e:
                    logger.error(f"Error converting relative links: {str(e)}")
                    return html_content
            
            html = convert_relative_links(html, base_url)
            logger.info(f"Converted relative links to absolute URLs using base: {base_url}")
        
        # Continue with existing metadata extraction logic
        
        # If that fails, try to find a domain in the content
        if not base_url:
            domain_match = re.search(r'\*\*URL\*\*: ([^\s\n]+\.[a-zA-Z]{2,}[^\s\n]*)', md_content)
            if domain_match:
                domain = domain_match.group(1)
                # Add https:// to the domain if it doesn't have a protocol
                if not domain.startswith(('http://', 'https://')):
                    base_url = f"https://{domain}"
                else:
                    base_url = domain
                    
        # If we still don't have a URL, try to extract from the report title
        if not base_url:
            # Look for patterns like "Direct_Crawl_-_https___google_com_"
            title_url_match = re.search(r'_-_(https?[_]+[^_]+)', report_title)
            if title_url_match:
                # Extract the URL pattern and replace _ with proper URL characters
                encoded_url = title_url_match.group(1)
                # Replace ___ with :// and _ with /
                decoded_url = encoded_url.replace('___', '://').replace('_', '/')
                base_url = decoded_url
            else:
                # Try to find just a domain name in the title
                domain_in_title = re.search(r'_([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})_', report_title)
                if domain_in_title:
                    domain = domain_in_title.group(1)
                    base_url = f"https://{domain}"
        
        # Extract generation date from report content
        date_match = re.search(r'\*Generated on ([^*]+)\*', md_content)
        generation_date = date_match.group(1) if date_match else datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # Create a properly cleaned report title
        if report_title.endswith('_' + ''.join(c for c in generation_date if c.isdigit())):
            # Remove timestamp suffix if present
            clean_title = report_title[:-(len(generation_date) + 1)]
        else:
            clean_title = report_title
            
        # Clean up title formatting
        clean_title = clean_title.replace('_', ' ').title()
        
        # Create "crawl again" link - use clean URL without escaping
        if base_url:
            crawl_again_link = f"/crawl?q={base_url}"
        else:
            crawl_again_link = "/crawl"
        
        # Extract JS info
        js_enabled = False
        js_info_match = re.search(r'\*\*JavaScript\*\*: (Enabled|Disabled)', md_content)
        if js_info_match:
            js_enabled = js_info_match.group(1) == 'Enabled'
        
        # Get favicon path
        favicon_path = "/static/images/favicon.ico"
        
        # Load and render the template
        try:
            template = jinja_env.get_template('reports/report_html.html')
            styled_html = template.render(
                clean_title=clean_title,
                favicon_path=favicon_path,
                generation_date=generation_date,
                base_url=base_url,
                js_enabled=js_enabled,
                crawl_again_link=crawl_again_link,
                markdown_file_basename=os.path.basename(markdown_file),
                html_content=html
            )
            logger.info("Created styled HTML document using template with lazy-loaded screenshots")
        except Exception as template_error:
            logger.error(f"Error loading template: {str(template_error)}")
            # Fallback to simple HTML if template fails
            styled_html = f"""<!DOCTYPE html>
<html>
<head>
    <title>{clean_title} - Gnosis Wraith Report</title>
    <meta charset="UTF-8">
</head>
<body>
    <h1>{clean_title}</h1>
    <p>Generated on: {generation_date}</p>
    <div>{html}</div>
</body>
</html>"""
        
        # Write HTML to file (moved outside exception handler)
        async with aiofiles.open(html_file, 'w', encoding='utf-8') as f:
            await f.write(styled_html)
            logger.info(f"HTML file written to {html_file}")
        
        return html_file
    except Exception as style_error:
        logger.error(f"Error creating styled HTML: {str(style_error)}")
        styled_html = f"<html><body><h1>Error Creating Styled HTML</h1><p>{str(style_error)}</p><div>{html}</div></body></html>"
        
        # Write HTML to file
        async with aiofiles.open(html_file, 'w', encoding='utf-8') as f:
            await f.write(styled_html)
            logger.info(f"HTML file written to {html_file}")
        
        return html_file
    except Exception as e:
        logger.error(f"Error in HTML conversion: {str(e)}")
        # Create a minimal error HTML file so we still have something to return
        error_html_path = f"{os.path.splitext(markdown_file)[0]}_error.html"
        try:
            with open(error_html_path, 'w') as f:
                f.write(f"<html><body><h1>Error Converting to HTML</h1><p>{str(e)}</p></body></html>")
            return error_html_path
        except Exception:
            logger.error(f"Could not even create error HTML")
            return markdown_file  # Return original markdown file as fallback