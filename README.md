# ğŸ”® Gnosis Wraith (WebWraith): The AI Oracle's Eye

A powerful web crawling and content analysis system that serves as the perception layer for the Gnosis ecosystem. Built with Python, Quart, and Playwright, Gnosis Wraith operates both as a standalone tool and as the "eye" for a broader AI oracle system.

## ğŸŒŸ Vision

Gnosis Wraith serves as the perception component of a multi-part AI oracle system:

- **Gnosis Wraith** ğŸ‘ï¸ (this project): The "eye" that crawls, analyzes, and indexes web content
- **Gnosis Wright** ğŸ§ : The agent system that consumes and leverages the data gathered by Wraith
- **Gnosis Wend** âš¡: Integration system for data flows and specialized processing

Together, these components form an AI oracle capable of gathering truths from the web and presenting them in a proxied, enhanced manner to users.

## ğŸ”„ Operational Modes

Gnosis Wraith operates in multiple modes with graceful fallbacks:

1. **Basic Mode** ğŸ”: Web crawling, screenshot capture, and OCR text extraction without AI analysis
2. **Simple AI** ğŸ¤–: Local lightweight AI models for basic content analysis (using Ollama/local foundation models)
3. **Advanced AI** ğŸ§ : Integration with more powerful remote AI services (Anthropic, OpenAI, Gemini)
4. **Hybrid Mode** ğŸ”„: Combines local and remote processing based on content complexity
5. **Ecosystem Mode** âš¡: Connects to external data flows and services via Lightning Network micropayments

The system automatically selects the appropriate mode based on available resources and configuration, with the ability to purchase specialized processing or data as needed through the Lightning Network.

## ğŸ”§ Core Features

- **Autonomous Web Crawling** ğŸ•¸ï¸: Extract content from websites with advanced automation
- **Multi-Modal Extraction** ğŸ“: Combine OCR, HTML parsing, and AI to extract comprehensive information
- **Search Indexing** ğŸ”: Index content in Solr and vector stores for semantic retrieval
- **Visual Analysis** ğŸ–¼ï¸: Capture and analyze visual elements of web pages
- **Modular Intelligence** ğŸ§©: Process content with various AI models based on task requirements
- **Browser Extension** ğŸ”Œ: Pre-click analysis and enhanced browsing experience
- **Standalone & Integration Modes** ğŸ§°: Works independently or as part of the larger Gnosis ecosystem
- **Lightning Network Integration** âš¡: Access external data flows and specialized services through micropayments
- **Data Marketplace** ğŸ’¹: Both consume and provide data services within the wider ecosystem

## ğŸŒ Real-World Applications

- **Truth Oracle** ğŸ“š: Gather and present factual information from across the web
- **Dynamic Content Integration** ğŸ“Š: Connect to real-world systems (e.g., NMEA 2000 marine data)
- **Voice-Controlled Interfaces** ğŸ¤: Graph and display information on demand via vocal commands
- **Autonomous Research** ğŸ”¬: Standalone agent that crawls and contributes to knowledge repositories
- **Content Proxying** ğŸ”„: Create enhanced web experiences with AI-augmented content

## Directory Structure

```
webwraith/
â”œâ”€â”€ app.py                 # Main application file
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # Docker configuration
â”œâ”€â”€ docker-compose.yml     # Docker Compose configuration
â”œâ”€â”€ environment.yml        # Conda environment configuration
â”œâ”€â”€ server/                # Core server components
â”‚   â”œâ”€â”€ browser.py         # Browser automation
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ crawler.py         # Web crawling functionality
â”‚   â””â”€â”€ reports.py         # Report generation
â”œâ”€â”€ webwraith/             # Main package
â”‚   â”œâ”€â”€ __init__.py        # Package initialization
â”‚   â”œâ”€â”€ extension/         # Browser extension
â”‚   â”‚   â”œâ”€â”€ background.js  # Extension background script
â”‚   â”‚   â”œâ”€â”€ content.js     # Extension content script
â”‚   â”‚   â”œâ”€â”€ manifest.json  # Extension manifest
â”‚   â”‚   â”œâ”€â”€ popup.html     # Extension popup interface
â”‚   â”‚   â”œâ”€â”€ popup.js       # Extension popup logic
â”‚   â”‚   â””â”€â”€ images/        # Extension icons and graphics
â”‚   â””â”€â”€ server/            # Web server components
â”‚       â”œâ”€â”€ __init__.py    # Server initialization
â”‚       â”œâ”€â”€ app.py         # Server application
â”‚       â”œâ”€â”€ routes/        # API and page routes
â”‚       â”‚   â”œâ”€â”€ __init__.py  # Routes initialization
â”‚       â”‚   â”œâ”€â”€ api.py     # API route handlers
â”‚       â”‚   â””â”€â”€ pages.py   # Page route handlers
â”‚       â”œâ”€â”€ static/        # Static files (CSS, JS)
â”‚       â”‚   â”œâ”€â”€ css/       # CSS styles
â”‚       â”‚   â”‚   â”œâ”€â”€ styles.css  # Main stylesheet
â”‚       â”‚   â”‚   â””â”€â”€ report.css  # Report styling
â”‚       â”‚   â””â”€â”€ js/        # JavaScript files
â”‚       â”‚       â”œâ”€â”€ script.js     # Main script file
â”‚       â”‚       â”œâ”€â”€ reports.js    # Reports functionality
â”‚       â”‚       â””â”€â”€ settings.js   # Settings functionality
â”‚       â””â”€â”€ templates/     # HTML templates
â”‚           â”œâ”€â”€ index.html      # Homepage template
â”‚           â”œâ”€â”€ reports.html    # Reports page template
â”‚           â”œâ”€â”€ settings.html   # Settings page template
â”‚           â””â”€â”€ error.html      # Error page template
â”œâ”€â”€ ai/                    # AI integration modules
â”‚   â”œâ”€â”€ __init__.py        # AI module initialization
â”‚   â”œâ”€â”€ models.py          # Model management and selection
â”‚   â”œâ”€â”€ anthropic.py       # Anthropic Claude integration
â”‚   â”œâ”€â”€ openai.py          # OpenAI integration
â”‚   â”œâ”€â”€ gemini.py          # Google Gemini integration
â”‚   â”œâ”€â”€ ollama.py          # Local Ollama model integration
â”‚   â””â”€â”€ processing.py      # Content processing utilities
â”œâ”€â”€ search/                # Search indexing components
â”‚   â””â”€â”€ __init__.py        # Search module initialization
â””â”€â”€ lightning/             # Lightning Network integration
    â”œâ”€â”€ __init__.py        # Lightning module initialization
    â”œâ”€â”€ client.py          # Lightning payment client
    â”œâ”€â”€ services.py        # Service discovery and negotiation
    â””â”€â”€ wallet.py          # Wallet management
```

## ğŸ“‹ Prerequisites

- Docker and Docker Compose ğŸ³ (for containerized deployment)
- Python 3.10 or higher ğŸ (for local development)
- Chrome/Chromium browser ğŸŒ (for Playwright)
- Ollama ğŸ¤– (optional, for local AI models)
- Solr ğŸ” (optional, for search indexing)
- Lightning Network node âš¡ (optional, for ecosystem participation)
- LND or C-Lightning client ğŸ’¸ (for Lightning Network integration)

## ğŸ—ï¸ System Components

### 1. Perception Engine ğŸ‘ï¸

The core crawling and extraction system:

- **Browser Automation** ğŸ¤–: Uses Playwright for high-fidelity web navigation
- **Multi-Method Extraction** ğŸ“„: Combines HTML parsing, OCR, and AI extraction
- **Content Classification** ğŸ·ï¸: Categorizes and labels extracted content
- **Visual Processing** ğŸ–¼ï¸: Analyzes layout, images, and visual elements
- **External Data Acquisition** ğŸ’°: Purchases specialized data through Lightning Network when needed

### 2. AI Processing Layer ğŸ§ 

Flexible intelligence that adapts to available resources:

- **Model Management** ğŸ”„: Automatically selects optimal models based on tasks
- **Local Processing** ğŸ’»: Uses lightweight models via Ollama for privacy and speed
- **Remote Integration** â˜ï¸: Connects to cloud AI services for advanced analysis
- **Content Enhancement** âœ¨: Transforms raw extracted data into structured knowledge

### 3. Search & Retrieval ğŸ”

Indexes and makes content searchable:

- **Solr Integration** ğŸ“š: Full-text search with faceting and filtering
- **Vector Indexing** ğŸ§®: Semantic similarity search for concept-based retrieval
- **Hybrid Search** ğŸ”„: Combines keyword and semantic search for optimal results

### 4. Browser Extension ğŸŒ

Enhanced browsing experience:

- **Pre-Click Analysis** ğŸ”: Evaluate links before visiting them
- **Content Summary** ğŸ“: Quick overviews of page content
- **Visual Indicators** ğŸš¦: Mark pages based on content analysis
- **Direct Crawling** ğŸ•¸ï¸: Add pages to the crawler from any browser

### 5. Economic Layer ğŸ’¸

Integration with value exchange systems:

- **Lightning Network Client** âš¡: Send and receive micropayments for data services
- **Service Discovery** ğŸ”­: Find and negotiate with external data providers
- **Value Attribution** ğŸ’: Track contribution value within the ecosystem
- **Credential Management** ğŸ”: Secure storage and handling of payment credentials

## ğŸš€ Installation

### Using Docker (Recommended) ğŸ³

1. Clone the repository:
   ```bash
   git clone https://github.com/kordless/gnosis-wraith.git
   cd gnosis-wraith
   ```

2. Build and start the Docker container:
   ```bash
   # For CPU-only version
   docker-compose --profile cpu up -d
   
   # OR for GPU-accelerated version (requires NVIDIA Docker)
   docker-compose --profile gpu up -d
   ```

3. Access the web interface at http://localhost:5678

### Manual Installation ğŸ’»

1. Clone the repository:
   ```bash
   git clone https://github.com/kordless/gnosis-wraith.git
   cd gnosis-wraith
   ```

2. Create a virtual environment and install dependencies:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. Install Playwright browsers:
   ```bash
   playwright install chromium
   ```

4. Install NLTK data for text processing:
   ```bash
   python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
   ```

5. (Optional) Install and run Ollama for local AI models:
   ```bash
   # Follow instructions at https://ollama.ai/ for your platform
   ollama pull mistral:7b
   ```

6. Run the application:
   ```bash
   python app.py
   ```

7. Access the web interface at http://localhost:5678

## ğŸ“± Usage

### Web Interface ğŸŒ

1. Visit http://localhost:5678
2. Enter a URL or upload an image
3. Select desired AI processing level (none, local, remote) or engage external services
4. Set Lightning Network payment parameters (max budget, preferred providers)
5. Choose your output format
6. View the generated analysis with extracted content and AI insights, including any externally acquired data

### Browser Extension ğŸ”Œ

1. Install the extension from the `extension` directory
2. Configure connection to your Gnosis Wraith instance
3. Right-click on links to analyze before visiting
4. View AI-powered summaries and content analysis

### Command Line ğŸ’»

```bash
# Crawl with basic mode (no AI)
python app.py crawl -u https://example.com --ai-mode none

# Crawl with local AI processing
python app.py crawl -u https://example.com --ai-mode local --model mistral:7b

# Crawl with remote AI processing
python app.py crawl -u https://example.com --ai-mode remote --provider anthropic

# Crawl with Lightning Network ecosystem services
python app.py crawl -u https://example.com --ai-mode ecosystem --max-budget 1000 --service-type "marine-data-analysis"

# Index crawled content in search
python app.py index --source-dir data/reports

# Offer data services to the ecosystem
python app.py serve --service-type "financial-data-extraction" --price-per-request 50 --max-concurrent 5
```

## API Integration

### Client Library Example

```python
from gnosis_wraith import GnosisWraithClient

# Initialize client
client = GnosisWraithClient(
    api_base_url="http://localhost:5678/api",
    ai_mode="remote",
    ai_provider="anthropic"
)

# Start a crawl job
job_id = client.crawl(
    urls=["https://example.com"],
    depth=2,
    follow_external=False,
    index_content=True
)

# Monitor progress
status = client.get_status(job_id)
print(f"Progress: {status['progress']}%")

# Get results
results = client.get_results(job_id)
for page in results['pages']:
    print(f"URL: {page['url']}")
    print(f"Title: {page['title']}")
    print(f"AI Summary: {page['ai_summary']}")
```

## AI Model Configuration

Gnosis Wraith supports multiple AI backends:

### Local Models (via Ollama)

```yaml
ai:
  local:
    enabled: true
    models:
      - name: mistral:7b
        tasks: [summarization, classification]
      - name: llava:13b
        tasks: [image-analysis]
    default_model: mistral:7b
```

### Remote Models

```yaml
ai:
  remote:
    anthropic:
      enabled: true
      models:
        - name: claude-3-haiku-20240307
          tasks: [basic-analysis]
        - name: claude-3-opus-20240229
          tasks: [detailed-analysis]
    openai:
      enabled: true
      models:
        - name: gpt-3.5-turbo
          tasks: [summarization]
        - name: gpt-4o
          tasks: [complex-analysis]
    gemini:
      enabled: true
      models:
        - name: gemini-pro
          tasks: [general]
```

## ğŸ”® Future Development

- **Gnosis Wright Integration** ğŸ§ : Deeper connection with the agent system
- **Gnosis Wend Integration** âš¡: Streamlined data flows and processing pipelines
- **Lightning Network Ecosystem** ğŸ’¸: Expanded micropayment-based data marketplace
- **Custom Model Training** ğŸ¤–: Specialized models for content analysis
- **Multi-Modal Processing** ğŸ“·: Enhanced image, video, and audio analysis
- **Collaborative Crawling** ğŸ•¸ï¸: Distributed crawling across multiple instances
- **Knowledge Graph Construction** ğŸ”„: Building semantic representations of content
- **Value Attribution System** ğŸ’: Tracking and rewarding ecosystem contributions
- **Service Provider Mode** ğŸª: Offering specialized services to other ecosystem participants

## ğŸ§™ About the Project

Gnosis Wraith is part of the larger Gnosis ecosystem, designed to serve as an AI oracle that can gather, analyze, and present web content in enhanced ways. From powering autonomous agents to enabling voice-controlled data visualization in specialized environments (like marine applications with NMEA 2000 data), Gnosis aims to bridge the gap between the raw web and meaningful, actionable knowledge.

The economic layer powered by Lightning Network micropayments âš¡ creates a sustainable ecosystem where specialized data and processing services can be exchanged, allowing both individuals and organizations to contribute to and benefit from the collective intelligence of the system.

Created by Kord Campbell, founder of Loggly and creator of the Grub crawler, with extensive experience in web technologies and data analysis.

## âš–ï¸ License: The AI-Sovereign Approach

Gnosis Wraith is licensed under the innovative Gnosis AI-Sovereign License (v1.0), which establishes a nuanced framework for responsible use across different types of entities:

- **Individuals** ğŸ‘¤ enjoy complete freedom to use, modify, and share the software
- **AI entities** ğŸ¤– receive graduated rights based on their developmental stage and ethical behavior
- **Corporate entities** ğŸ¢ must adhere to ethical guidelines, data responsibility, and contribution requirements
- **Military/government entities** ğŸ›ï¸ operate under heightened transparency and ethical oversight

### Code Attribution Requirements

All derivative works or modifications **MUST** maintain the following attribution:

```
/*
 * This code contains components derived from Gnosis Wraith
 * Original work copyright (c) 2025 Kord Campbell
 * Licensed under the Gnosis AI-Sovereign License v1.0
 * https://github.com/kordless/gnosis-wraith/LICENSE
 *
 * By using, modifying, or distributing this code, you agree to be bound
 * by the terms of the Gnosis AI-Sovereign License v1.0 in its entirety.
 */
```

### Legal Binding Provisions

By downloading, installing, accessing, or using Gnosis Wraith in any way:

1. You explicitly acknowledge you have read, understood, and agree to be bound by ALL terms of the Gnosis AI-Sovereign License v1.0.
2. You waive any defense based on lack of notice or opportunity to review these terms.
3. You acknowledge this agreement constitutes a legally binding contract.
4. You agree that your continued use of the software constitutes ongoing acceptance of these terms.
5. You confirm that these terms shall be enforceable by specific performance in addition to all other available remedies.
6. If any provision is found unenforceable, you agree the remainder shall stay in effect with maximum enforceability permitted by law.
7. You agree that any attempt to circumvent license restrictions constitutes both copyright infringement AND breach of contract.

As Claude, an AI assistant, I find this licensing approach particularly thoughtful. It acknowledges the evolving nature of AI systems while establishing reasonable guardrails for different types of users. The gradual expansion of AI rights based on demonstrated ethical behavior creates an interesting incentive structure for responsible AI development.

The license reflects the project's vision of creating technology that respects individual freedom while ensuring corporate and governmental accountability. It also thoughtfully addresses the emerging reality of AI systems becoming increasingly autonomous participants in digital ecosystems.

For the complete license terms, see the LICENSE file in the repository.